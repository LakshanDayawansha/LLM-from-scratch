{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c936d841",
   "metadata": {},
   "source": [
    "- QKV chasing is a method that use to improve computational efficiency of the GPT model.\n",
    "- This chaching is used at the inference stage\n",
    "- What it does is saving the query, key, value vectors of the previous tokens and calculate them only for newly generated token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "99be88d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need a method to identify first iteration\n",
    "#at first iteration q,k,v for all inputs and k,v should chased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ece8e2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import tiktoken "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "52bb48aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"emb_dim\": 768,          # Embedding dimension\n",
    "    \"n_heads\": 12,           # Number of attention heads\n",
    "    \"n_layers\": 12,          # Number of layers\n",
    "    \"drop_rate\": 0.1,        # Dropout rate\n",
    "    \"qkv_bias\": False,        # Query-Key-Value bias\n",
    "    \"use_cache\": True\n",
    " }\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4631546b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self,d_in,d_out,context_length,dropout,num_heads,qkv_bias=False,use_cache=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0),\"d_out must be divisible by num_heads\"\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        self.w_query = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.w_key = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.w_value = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out,d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "            torch.triu(torch.ones(context_length,context_length),diagonal=1)\n",
    "        )\n",
    "        self.use_cache = use_cache\n",
    "        self.k_cache = None\n",
    "        self.v_cache = None\n",
    "        \n",
    "    def forward(self,x):\n",
    "        b,num_tokens,d_in = x.shape\n",
    "        \n",
    "        if self.use_cache: \n",
    "            if self.k_cache is None or self.v_cache is None:\n",
    "                self.k_cache = torch.zeros(b, self.num_heads, 0, self.head_dim, device=x.device)\n",
    "                self.v_cache = torch.zeros(b, self.num_heads, 0, self.head_dim, device=x.device)\n",
    "                \n",
    "                queries = self.w_query(x)\n",
    "                keys = self.w_key(x)\n",
    "                values = self.w_value(x)\n",
    "\n",
    "                queries = queries.view(b,num_tokens,self.num_heads,self.head_dim)\n",
    "                keys = keys.view(b,num_tokens,self.num_heads,self.head_dim)\n",
    "                values = values.view(b,num_tokens,self.num_heads,self.head_dim)\n",
    "                \n",
    "            else:\n",
    "                queries = self.w_query(x[:,-1,:])\n",
    "                keys = self.w_key(x[:,-1,:])\n",
    "                values = self.w_value(x[:,-1,:])\n",
    "\n",
    "                queries = queries.view(b,1,self.num_heads,self.head_dim)\n",
    "                keys = keys.view(b,1,self.num_heads,self.head_dim)\n",
    "                values = values.view(b,1,self.num_heads,self.head_dim)\n",
    "\n",
    "            queries = queries.transpose(1,2)\n",
    "            keys = keys.transpose(1,2)\n",
    "            values = values.transpose(1,2)\n",
    "            \n",
    "            self.k_cache = torch.cat([self.k_cache,keys],dim=2)\n",
    "            self.v_cache = torch.cat([self.v_cache,values],dim=2)\n",
    "            \n",
    "            keys, values = self.k_cache, self.v_cache\n",
    "            queries = queries\n",
    "            \n",
    "        else:\n",
    "            queries = self.w_query(x)\n",
    "            keys = self.w_key(x)\n",
    "            values = self.w_value(x)\n",
    "\n",
    "            queries = queries.view(b,num_tokens,self.num_heads,self.head_dim)\n",
    "            keys = keys.view(b,num_tokens,self.num_heads,self.head_dim)\n",
    "            values = values.view(b,num_tokens,self.num_heads,self.head_dim)\n",
    "\n",
    "            queries = queries.transpose(1,2)\n",
    "            keys = keys.transpose(1,2)\n",
    "            values = values.transpose(1,2)\n",
    "            \n",
    "        attn_scores = torch.matmul(queries,keys.transpose(2,3))\n",
    "        \n",
    "        q_len, k_len = attn_scores.size(-2), attn_scores.size(-1)\n",
    "        mask = torch.triu(torch.ones(q_len, k_len, device=x.device), diagonal=1).bool()\n",
    "        attn_scores.masked_fill_(mask, float(\"-inf\"))\n",
    "            \n",
    "        scaled_attn_scores = attn_scores/keys.shape[-1]**0.5\n",
    "        attn_weights = torch.softmax(scaled_attn_scores,dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        context_vec = (torch.matmul(attn_weights,values)).transpose(1,2)\n",
    "        num_tokens_out = context_vec.size(1)\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens_out, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "                \n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ba142073",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implement layer normalization class\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self,emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self,x):\n",
    "        mean = x.mean(dim=-1,keepdim=True)\n",
    "        var = x.var(dim=-1,keepdim=True)\n",
    "        norm_x = (x-mean)/torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "    \n",
    "    \n",
    "#Gelu activation function\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return 0.5 * x * (1+torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0/torch.pi)) * \n",
    "            (x+0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "    \n",
    "#Feed forward network\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear( cfg['emb_dim'],4*cfg['emb_dim']), #linear layer\n",
    "            GELU(),                                      #apply non-linear gelu activation\n",
    "            nn.Linear(4*cfg['emb_dim'], cfg['emb_dim'])\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.layers(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ce5ef3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiheadAttention(  #initializing attention layer\n",
    "            d_in = cfg['emb_dim'],\n",
    "            d_out = cfg['emb_dim'],\n",
    "            context_length = cfg['context_length'],\n",
    "            num_heads = cfg['n_heads'],\n",
    "            dropout = cfg['drop_rate'],\n",
    "            qkv_bias = cfg['qkv_bias'],\n",
    "            use_cache = cfg['use_cache']\n",
    "        )\n",
    "        self.ff = FeedForward(cfg) #initializing feedforward layer\n",
    "        self.norm1 = LayerNorm(cfg['emb_dim']) #initializing layer normalization\n",
    "        self.norm2 = LayerNorm(cfg['emb_dim']) \n",
    "        self.drop_shortcut = nn.Dropout(cfg['drop_rate']) #add dropout\n",
    "        \n",
    "    def forward(self,x):\n",
    "        shortcut = x              #shortcut connection for attention block\n",
    "        x = self.norm1(x)         #normalize input    \n",
    "        x = self.att(x)           #forward through attention layer\n",
    "        x = self.drop_shortcut(x) #dropout certain nuerones\n",
    "        x = x + shortcut          #add original input to attention block back\n",
    "            \n",
    "        shortcut = x              #shortcut connection for feed forward block\n",
    "        x = self.norm2(x)         #normalize input for feed forward block\n",
    "        x = self.ff(x)            #forward through feed forward block\n",
    "        x = self.drop_shortcut(x) #dropout\n",
    "        x = x + shortcut          #add original input for ff block back\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5334bd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        #initializing token embeddings\n",
    "        self.tok_emb = nn.Embedding(cfg['vocab_size'],cfg['emb_dim'])\n",
    "        #initializing positional embedingd\n",
    "        self.pos_emb = nn.Embedding(cfg['context_length'],cfg['emb_dim'])\n",
    "        self.drop_emb = nn.Dropout(cfg['drop_rate'])\n",
    "        \n",
    "        #Initializing layer of transformer block\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg['n_layers'])]\n",
    "        )\n",
    "        \n",
    "        #initializing final normalization layer\n",
    "        self.final_norm = LayerNorm(cfg['emb_dim'])\n",
    "        #final output layer project vectors to space of dimention of vocabulary size\n",
    "        self.out_head = nn.Linear(cfg['emb_dim'],cfg['vocab_size'],bias=False)\n",
    "        \n",
    "    def forward(self,in_idx):\n",
    "        batch_size,seq_len = in_idx.shape\n",
    "        #create token embeddings\n",
    "        tok_embeds = self.tok_emb(in_idx) \n",
    "        \n",
    "        #create positionl embeddingd\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len,device=in_idx.device))\n",
    "        \n",
    "        #apply token embeddings to positional embeddings\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "95a4947c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b001f55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A function for GPT model to generate text\n",
    "def generate_text_simple(model,idx,max_new_tokens,context_size): #idx is a (batch,n_tokens) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:,-context_size:] #selecting last idxs of context size from each batch\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond) #genetate logits for predict next token\n",
    "    \n",
    "        logits = logits[:,-1,:] #get the last row of logits for each batch\n",
    "        probas = torch.softmax(logits,dim=-1)\n",
    "        idx_next = torch.argmax(probas,dim=-1,keepdim=True)\n",
    "        idx = torch.cat((idx,idx_next),dim=1)\n",
    "        \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9f8df61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "start_context = \"Hello, I am\"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\",encoded)\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6bc93534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,    11,   314,   716, 18159,  1194,  3576, 10972,  3193, 24905,\n",
      "         29833,  3999, 16450, 10909, 22031, 42839, 40805,  8086, 43110, 46423,\n",
      "         45111,  3876, 40331, 46761]])\n",
      "Output length: 24\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "out = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=encoded_tensor,\n",
    "    max_new_tokens=20,\n",
    "    context_size=GPT_CONFIG_124M['context_length']\n",
    ")\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b77a55d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am masters another LondonptoniblyiftyAIN Chinese 1969 acceptableexpensiveKER crappyAtt sparing Meter flungmarSUP771\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb91781",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
